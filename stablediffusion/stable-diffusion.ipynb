{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec7e77-8978-4076-bfe0-7a05e4ed5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/k-diffusion\n",
    "\n",
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "import accelerate\n",
    "\n",
    "import k_diffusion as K\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "ckpt = \"/weights/sd-v1-3-full-ema.ckpt\"\n",
    "config = \"/workspace/k-diffusion/v1-inference.yaml\"\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False, device='cuda'):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model = model.half().to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "class CFGDenoiser(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.inner_model = model\n",
    "\n",
    "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        sigma_in = torch.cat([sigma] * 2)\n",
    "        cond_in = torch.cat([uncond, cond])\n",
    "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
    "        return uncond + (cond - uncond) * cond_scale\n",
    "\n",
    "def do_run(accelerator, device, model, config, opt):\n",
    "    from types import SimpleNamespace\n",
    "    opt = SimpleNamespace(**opt)\n",
    "    seed_everything(opt.seed)\n",
    "    seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
    "    torch.manual_seed(seeds[accelerator.process_index].item())\n",
    "\n",
    "    if opt.plms:\n",
    "        sampler = PLMSSampler(model)\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "\n",
    "    model_wrap = K.external.CompVisDenoiser(model)\n",
    "    sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()\n",
    "\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    outpath = opt.outdir\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "    if not opt.from_file:\n",
    "        prompt = opt.prompt\n",
    "        assert prompt is not None\n",
    "        data = [batch_size * [prompt]]\n",
    "\n",
    "    else:\n",
    "        print(f\"reading prompts from {opt.from_file}\")\n",
    "        with open(opt.from_file, \"r\") as f:\n",
    "            data = f.read().splitlines()\n",
    "            data = list(chunk(data, batch_size))\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "    grid_count = len(os.listdir(outpath)) - 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.ema_scope():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                for n in trange(opt.n_iter, desc=\"Sampling\", disable=not accelerator.is_main_process):\n",
    "                    for prompts in tqdm(data, desc=\"data\", disable=not accelerator.is_main_process):\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "                        shape = [opt.C, opt.H//opt.f, opt.W//opt.f]\n",
    "                        sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
    "                        x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
    "                        model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                        extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
    "                        samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
    "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "                        x_samples_ddim = accelerator.gather(x_samples_ddim)\n",
    "\n",
    "                        if accelerator.is_main_process and not opt.skip_save:\n",
    "                            for x_sample in x_samples_ddim:\n",
    "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                                Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                                base_count += 1\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "                if accelerator.is_main_process and not opt.skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                    # to image\n",
    "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "                    grid_count += 1\n",
    "\n",
    "                toc = time.time()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "            f\"Sampling took {toc-tic:g}s, i.e. produced {opt.n_iter * opt.n_samples * accelerator.num_processes / (toc - tic):.2f} samples/sec.\")\n",
    "        display(Image.fromarray(grid.astype(np.uint8)))\n",
    "    \n",
    "accelerator = accelerate.Accelerator()\n",
    "device = accelerator.device\n",
    "config = OmegaConf.load(f\"{config}\")\n",
    "model = load_model_from_config(config, f\"{ckpt}\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154dfd08-cf5d-4dc0-880f-246908b76525",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    \"prompt\" : \"A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood by greg rutkowski and thomas kinkade, Trending on artstation.\",\n",
    "    \"outdir\" : \"/out\",\n",
    "    \"skip_grid\" : False,\n",
    "    \"skip_save\" : False,\n",
    "    \"ddim_steps\" : 50,\n",
    "    \"plms\" : False,\n",
    "    \"ddim_eta\" : 0.0,\n",
    "    \"n_iter\" : 1,\n",
    "    \"W\" : 512,\n",
    "    \"H\" : 512,\n",
    "    \"C\" : 4,\n",
    "    \"f\" : 8,\n",
    "    \"n_samples\" : 4,\n",
    "    \"n_rows\" : 2,\n",
    "    \"scale\" : 5.0,\n",
    "    \"dyn\" : None,\n",
    "    \"from_file\": None,\n",
    "    \"seed\" : 324\n",
    "}\n",
    "do_run(accelerator, device, model, config, opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
